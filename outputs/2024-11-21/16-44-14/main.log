[2024-11-21 16:44:14,823][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2024-11-21 16:44:23,528][flwr][INFO] - Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:__internal_head__': 1.0, 'object_store_memory': 1447621017.0, 'node:127.0.0.1': 1.0, 'memory': 2895242036.0, 'GPU': 1.0}
[2024-11-21 16:44:23,528][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-11-21 16:44:23,528][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-11-21 16:44:23,550][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[2024-11-21 16:44:23,552][flwr][INFO] - Initializing global parameters
[2024-11-21 16:44:23,553][flwr][INFO] - Requesting initial parameters from one random client
[2024-11-21 16:44:30,762][flwr][INFO] - Received initial parameters from one random client
[2024-11-21 16:44:30,762][flwr][INFO] - Evaluating initial parameters
[2024-11-21 16:44:33,678][flwr][INFO] - initial parameters (loss, other metrics): 182.38459253311157, {'accuracy': 0.1304}
[2024-11-21 16:44:33,679][flwr][INFO] - FL starting
[2024-11-21 16:44:33,680][flwr][DEBUG] - fit_round 1: strategy sampled 10 clients (out of 100)
[2024-11-21 16:44:51,866][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\serialization.py", line 623, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\serialization.py", line 767, in _legacy_save
    storage._write_file(f, _should_read_directly(f), True, torch._utils._element_size(dtype))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 408, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 392, in process_unordered_future
    self._return_actor(actor)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\actor_pool.py", line 366, in _return_actor
    self.submit(*self._pending_submits.pop(0))
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 243, in submit
    future = fn(actor, client_fn, job_fn, cid)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 135, in <lambda>
    lambda a, c_fn, j_fn, cid: a.run.remote(c_fn, j_fn, cid),
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 687, in ray._raylet.prepare_args_internal
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 468, in serialize
    return self._serialize_to_msgpack(value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 446, in _serialize_to_msgpack
    pickle5_serialized_object = self._serialize_to_pickle5(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 408, in _serialize_to_pickle5
    raise e
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 403, in _serialize_to_pickle5
    inband = pickle.dumps(
             ^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 88, in dumps
    cp.dump(obj)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 733, in dump
    return Pickler.dump(self, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 904, in __reduce__
    torch.save(self, b, _use_new_zipfile_serialization=False)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\serialization.py", line 622, in save
    with _open_file_like(f, 'wb') as opened_file:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\serialization.py", line 430, in __exit__
    self.file_like.flush()
ValueError: I/O operation on closed file.

[2024-11-21 16:44:51,866][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 408, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 392, in process_unordered_future
    self._return_actor(actor)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\actor_pool.py", line 366, in _return_actor
    self.submit(*self._pending_submits.pop(0))
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 243, in submit
    future = fn(actor, client_fn, job_fn, cid)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 135, in <lambda>
    lambda a, c_fn, j_fn, cid: a.run.remote(c_fn, j_fn, cid),
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 687, in ray._raylet.prepare_args_internal
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 468, in serialize
    return self._serialize_to_msgpack(value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\serialization.py", line 442, in _serialize_to_msgpack
    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\includes/serialization.pxi", line 175, in ray._raylet.MessagePackSerializer.dumps
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\msgpack\__init__.py", line 36, in packb
    return Packer(**kwargs).pack(o)
           ^^^^^^^^^^^^^^^^
  File "msgpack\\_packer.pyx", line 120, in msgpack._cmsgpack.Packer.__cinit__
MemoryError: Unable to allocate internal buffer.

[2024-11-21 16:44:51,866][flwr][ERROR] - I/O operation on closed file.
[2024-11-21 16:44:51,866][flwr][ERROR] - Unable to allocate internal buffer.
[2024-11-21 16:45:07,698][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 72, in run
    job_results = job_fn(client)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1294, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1145, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 21536) exited unexpectedly

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 40 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1132, in _try_get_data\n    data = self._data_queue.get(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py", line 114, in get\n    raise Empty\n_queue.Empty\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1328, in _next_data\n    idx, data = self._get_data()\n                ^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1294, in _get_data\n    success, data = self._try_get_data()\n                    ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1145, in _try_get_data\n    raise RuntimeError(f\'DataLoader worker (pid(s) {pids_str}) exited unexpectedly\') from e\nRuntimeError: DataLoader worker (pid(s) 21536) exited unexpectedly\n',)

[2024-11-21 16:45:07,702][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\queues.py", line 114, in get
    raise Empty
_queue.Empty

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 72, in run
    job_results = job_fn(client)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1294, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1145, in _try_get_data
    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
RuntimeError: DataLoader worker (pid(s) 21536) exited unexpectedly

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=20032, ip=127.0.0.1, actor_id=d163ae5df0c8609cd55753e001000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000192E90D1090>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 40 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1132, in _try_get_data\n    data = self._data_queue.get(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\queues.py", line 114, in get\n    raise Empty\n_queue.Empty\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1328, in _next_data\n    idx, data = self._get_data()\n                ^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1294, in _get_data\n    success, data = self._try_get_data()\n                    ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1145, in _try_get_data\n    raise RuntimeError(f\'DataLoader worker (pid(s) {pids_str}) exited unexpectedly\') from e\nRuntimeError: DataLoader worker (pid(s) 21536) exited unexpectedly\n',)
[2024-11-21 16:45:43,506][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-11-21 16:45:43,506][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-11-21 16:45:43,506][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=c2ae2a8f908e476d752ca33901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000025A8B0EE5D0>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 438, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1039, in __init__
    w.start()
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\popen_spawn_win32.py", line 94, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\multiprocessing\reductions.py", line 417, in reduce_storage
    metadata = storage._share_filename_cpu_()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 297, in wrapper
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 334, in _share_filename_cpu_
    return super()._share_filename_cpu_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Couldn't open shared file mapping: <0000025AF559A362>, error code: <1455>

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=c2ae2a8f908e476d752ca33901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000025A8B0EE5D0>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 46 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 438, in __iter__\n    return self._get_iterator()\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 386, in _get_iterator\n    return _MultiProcessingDataLoaderIter(self)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1039, in __init__\n    w.start()\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py", line 121, in start\n    self._popen = self._Popen(self)\n                  ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py", line 224, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py", line 336, in _Popen\n    return Popen(process_obj)\n           ^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py", line 94, in __init__\n    reduction.dump(process_obj, to_child)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py", line 417, in reduce_storage\n    metadata = storage._share_filename_cpu_()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 297, in wrapper\n    return fn(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 334, in _share_filename_cpu_\n    return super()._share_filename_cpu_(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn\'t open shared file mapping: <0000025AF559A362>, error code: <1455>\n',)

[2024-11-21 16:45:43,506][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=24532, ip=127.0.0.1, actor_id=cd0ccbea8fb91548604dde8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000001BA52DF11D0>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 37, in train
    net.to(device)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1160, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 810, in _apply
    module._apply(fn)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 833, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24532, ip=127.0.0.1, actor_id=cd0ccbea8fb91548604dde8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000001BA52DF11D0>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 18 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 37, in train\n    net.to(device)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 1160, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 810, in _apply\n    module._apply(fn)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 833, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)

[2024-11-21 16:45:43,506][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-11-21 16:45:43,509][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-11-21 16:45:43,509][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=c2ae2a8f908e476d752ca33901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000025A8B0EE5D0>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 438, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 386, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1039, in __init__
    w.start()
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\popen_spawn_win32.py", line 94, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\multiprocessing\reductions.py", line 417, in reduce_storage
    metadata = storage._share_filename_cpu_()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 297, in wrapper
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 334, in _share_filename_cpu_
    return super()._share_filename_cpu_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Couldn't open shared file mapping: <0000025AF559A362>, error code: <1455>

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=c2ae2a8f908e476d752ca33901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000025A8B0EE5D0>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 46 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 438, in __iter__\n    return self._get_iterator()\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 386, in _get_iterator\n    return _MultiProcessingDataLoaderIter(self)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1039, in __init__\n    w.start()\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py", line 121, in start\n    self._popen = self._Popen(self)\n                  ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py", line 224, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py", line 336, in _Popen\n    return Popen(process_obj)\n           ^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py", line 94, in __init__\n    reduction.dump(process_obj, to_child)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py", line 417, in reduce_storage\n    metadata = storage._share_filename_cpu_()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 297, in wrapper\n    return fn(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 334, in _share_filename_cpu_\n    return super()._share_filename_cpu_(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn\'t open shared file mapping: <0000025AF559A362>, error code: <1455>\n',)
[2024-11-21 16:45:43,517][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=24532, ip=127.0.0.1, actor_id=cd0ccbea8fb91548604dde8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000001BA52DF11D0>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 37, in train
    net.to(device)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1160, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 810, in _apply
    module._apply(fn)
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 833, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=24532, ip=127.0.0.1, actor_id=cd0ccbea8fb91548604dde8401000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000001BA52DF11D0>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 18 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 37, in train\n    net.to(device)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 1160, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 810, in _apply\n    module._apply(fn)\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 833, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py", line 1158, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n',)
[2024-11-21 16:45:43,527][flwr][DEBUG] - fit_round 1 received 3 results and 7 failures
[2024-11-21 16:45:43,534][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2024-11-21 16:45:47,289][flwr][INFO] - fit progress: (1, 179.12574791908264, {'accuracy': 0.1962}, 73.61855570005719)
[2024-11-21 16:45:47,289][flwr][INFO] - evaluate_round 1: no clients selected, cancel
[2024-11-21 16:45:47,289][flwr][DEBUG] - fit_round 2: strategy sampled 10 clients (out of 100)
[2024-11-21 16:47:17,531][flwr][DEBUG] - fit_round 2 received 10 results and 0 failures
[2024-11-21 16:47:25,693][flwr][INFO] - fit progress: (2, 160.45483231544495, {'accuracy': 0.4818}, 172.01443600002676)
[2024-11-21 16:47:25,711][flwr][INFO] - evaluate_round 2: no clients selected, cancel
[2024-11-21 16:47:25,736][flwr][DEBUG] - fit_round 3: strategy sampled 10 clients (out of 100)
[2024-11-21 16:50:13,469][flwr][DEBUG] - fit_round 3 received 10 results and 0 failures
[2024-11-21 16:50:23,285][flwr][INFO] - fit progress: (3, 71.69466906785965, {'accuracy': 0.7066}, 349.606592600001)
[2024-11-21 16:50:23,287][flwr][INFO] - evaluate_round 3: no clients selected, cancel
[2024-11-21 16:50:23,289][flwr][DEBUG] - fit_round 4: strategy sampled 10 clients (out of 100)
[2024-11-21 16:52:05,488][flwr][DEBUG] - fit_round 4 received 10 results and 0 failures
[2024-11-21 16:52:14,648][flwr][INFO] - fit progress: (4, 46.59772431850433, {'accuracy': 0.8062}, 460.9698190999916)
[2024-11-21 16:52:14,652][flwr][INFO] - evaluate_round 4: no clients selected, cancel
[2024-11-21 16:52:14,654][flwr][DEBUG] - fit_round 5: strategy sampled 10 clients (out of 100)
[2024-11-21 16:54:13,729][flwr][DEBUG] - fit_round 5 received 10 results and 0 failures
[2024-11-21 16:54:23,249][flwr][INFO] - fit progress: (5, 37.05455194413662, {'accuracy': 0.8383}, 589.570198200061)
[2024-11-21 16:54:23,251][flwr][INFO] - evaluate_round 5: no clients selected, cancel
[2024-11-21 16:54:23,253][flwr][DEBUG] - fit_round 6: strategy sampled 10 clients (out of 100)
[2024-11-21 16:56:09,970][flwr][DEBUG] - fit_round 6 received 10 results and 0 failures
[2024-11-21 16:56:18,442][flwr][INFO] - fit progress: (6, 24.421848826110363, {'accuracy': 0.9111}, 704.7631589999655)
[2024-11-21 16:56:18,446][flwr][INFO] - evaluate_round 6: no clients selected, cancel
[2024-11-21 16:56:18,447][flwr][DEBUG] - fit_round 7: strategy sampled 10 clients (out of 100)
[2024-11-21 16:58:04,165][flwr][DEBUG] - fit_round 7 received 10 results and 0 failures
[2024-11-21 16:58:10,365][flwr][INFO] - fit progress: (7, 20.84569477662444, {'accuracy': 0.9245}, 816.6858415000606)
[2024-11-21 16:58:10,366][flwr][INFO] - evaluate_round 7: no clients selected, cancel
[2024-11-21 16:58:10,367][flwr][DEBUG] - fit_round 8: strategy sampled 10 clients (out of 100)
[2024-11-21 16:59:58,822][flwr][DEBUG] - fit_round 8 received 10 results and 0 failures
[2024-11-21 17:00:06,236][flwr][INFO] - fit progress: (8, 17.011265698820353, {'accuracy': 0.9397}, 932.5568977000657)
[2024-11-21 17:00:06,238][flwr][INFO] - evaluate_round 8: no clients selected, cancel
[2024-11-21 17:00:06,240][flwr][DEBUG] - fit_round 9: strategy sampled 10 clients (out of 100)
[2024-11-21 17:01:23,087][flwr][DEBUG] - fit_round 9 received 10 results and 0 failures
[2024-11-21 17:01:26,721][flwr][INFO] - fit progress: (9, 15.38092054054141, {'accuracy': 0.9409}, 1013.0418238999555)
[2024-11-21 17:01:26,722][flwr][INFO] - evaluate_round 9: no clients selected, cancel
[2024-11-21 17:01:26,723][flwr][DEBUG] - fit_round 10: strategy sampled 10 clients (out of 100)
[2024-11-21 17:02:18,916][flwr][DEBUG] - fit_round 10 received 10 results and 0 failures
[2024-11-21 17:02:21,520][flwr][INFO] - fit progress: (10, 12.495139569044113, {'accuracy': 0.9531}, 1067.8409140000585)
[2024-11-21 17:02:21,521][flwr][INFO] - evaluate_round 10: no clients selected, cancel
[2024-11-21 17:02:21,521][flwr][INFO] - FL finished in 1067.842215099954
[2024-11-21 17:02:21,522][flwr][INFO] - app_fit: losses_distributed []
[2024-11-21 17:02:21,523][flwr][INFO] - app_fit: metrics_distributed_fit {}
[2024-11-21 17:02:21,524][flwr][INFO] - app_fit: metrics_distributed {}
[2024-11-21 17:02:21,524][flwr][INFO] - app_fit: losses_centralized [(0, 182.38459253311157), (1, 179.12574791908264), (2, 160.45483231544495), (3, 71.69466906785965), (4, 46.59772431850433), (5, 37.05455194413662), (6, 24.421848826110363), (7, 20.84569477662444), (8, 17.011265698820353), (9, 15.38092054054141), (10, 12.495139569044113)]
[2024-11-21 17:02:21,525][flwr][INFO] - app_fit: metrics_centralized {'accuracy': [(0, 0.1304), (1, 0.1962), (2, 0.4818), (3, 0.7066), (4, 0.8062), (5, 0.8383), (6, 0.9111), (7, 0.9245), (8, 0.9397), (9, 0.9409), (10, 0.9531)]}
[2024-11-21 17:02:21,561][__main__][INFO] - Aplicao iniciada
