[2024-11-21 16:46:07,959][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2024-11-21 16:46:28,911][flwr][INFO] - Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'object_store_memory': 397221888.0, 'memory': 794443776.0, 'node:127.0.0.1': 1.0, 'CPU': 8.0, 'GPU': 1.0}
[2024-11-21 16:46:28,917][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-11-21 16:46:28,919][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-11-21 16:46:28,971][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[2024-11-21 16:46:28,973][flwr][INFO] - Initializing global parameters
[2024-11-21 16:46:28,973][flwr][INFO] - Requesting initial parameters from one random client
[2024-11-21 16:46:49,709][flwr][INFO] - Received initial parameters from one random client
[2024-11-21 16:46:49,711][flwr][INFO] - Evaluating initial parameters
[2024-11-21 16:46:57,768][flwr][INFO] - initial parameters (loss, other metrics): 182.52822732925415, {'accuracy': 0.0894}
[2024-11-21 16:46:57,830][flwr][INFO] - FL starting
[2024-11-21 16:46:57,884][flwr][DEBUG] - fit_round 1: strategy sampled 10 clients (out of 100)
[2024-11-21 16:48:32,882][flwr][DEBUG] - fit_round 1 received 10 results and 0 failures
[2024-11-21 16:48:32,911][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2024-11-21 16:48:40,302][flwr][INFO] - fit progress: (1, 179.48386120796204, {'accuracy': 0.2485}, 102.41876669996418)
[2024-11-21 16:48:40,304][flwr][INFO] - evaluate_round 1: no clients selected, cancel
[2024-11-21 16:48:40,304][flwr][DEBUG] - fit_round 2: strategy sampled 10 clients (out of 100)
[2024-11-21 16:50:33,552][flwr][DEBUG] - fit_round 2 received 10 results and 0 failures
[2024-11-21 16:50:39,246][flwr][INFO] - fit progress: (2, 164.87597584724426, {'accuracy': 0.4898}, 221.3681343999924)
[2024-11-21 16:50:39,246][flwr][INFO] - evaluate_round 2: no clients selected, cancel
[2024-11-21 16:50:39,246][flwr][DEBUG] - fit_round 3: strategy sampled 10 clients (out of 100)
[2024-11-21 16:51:51,052][flwr][DEBUG] - fit_round 3 received 10 results and 0 failures
[2024-11-21 16:51:58,088][flwr][INFO] - fit progress: (3, 68.83221188187599, {'accuracy': 0.739}, 300.2122524000006)
[2024-11-21 16:51:58,088][flwr][INFO] - evaluate_round 3: no clients selected, cancel
[2024-11-21 16:51:58,099][flwr][DEBUG] - fit_round 4: strategy sampled 10 clients (out of 100)
[2024-11-21 16:53:07,650][flwr][DEBUG] - fit_round 4 received 10 results and 0 failures
[2024-11-21 16:53:13,950][flwr][INFO] - fit progress: (4, 50.320767253637314, {'accuracy': 0.8152}, 376.07121820002794)
[2024-11-21 16:53:13,950][flwr][INFO] - evaluate_round 4: no clients selected, cancel
[2024-11-21 16:53:13,950][flwr][DEBUG] - fit_round 5: strategy sampled 10 clients (out of 100)
[2024-11-21 16:54:13,737][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 138, in _submit_job
    res = self.actor_pool.get_client_result(self.cid, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 414, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 300, in _fetch_future_result
    res_cid, res = ray.get(future)  # type: (str, ClientRes)
                   ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=10776, ip=127.0.0.1, actor_id=a61260e8d671e45a7091d0df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000273C7202A90>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1345, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1371, in _process_data
    data.reraise()
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\_utils.py", line 694, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 265, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 119, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 160, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 866, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 260, in _new_shared
    return cls._new_using_filename_cpu(size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Couldn't open shared file mapping: <000001EC9AE53EC2>, error code: <1455>


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=10776, ip=127.0.0.1, actor_id=a61260e8d671e45a7091d0df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000273C7202A90>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1345, in _next_data\n    return self._process_data(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1371, in _process_data\n    data.reraise()\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py", line 694, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 160, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 866, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 260, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn\'t open shared file mapping: <000001EC9AE53EC2>, error code: <1455>\n\n',)

[2024-11-21 16:54:13,760][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=10776, ip=127.0.0.1, actor_id=a61260e8d671e45a7091d0df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000273C7202A90>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 191, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\client\numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\client.py", line 68, in fit
    train(self.model, self.trainloader, optim, epochs, self.device)
  File "C:\Users\marco\Documents\Workplace\FL\Flower_Pytorch\Parte_1\model.py", line 39, in train
    for images, labels in trainloader:
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1345, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 1371, in _process_data
    data.reraise()
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\_utils.py", line 694, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 265, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 119, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\_utils\collate.py", line 160, in collate_tensor_fn
    storage = elem._typed_storage()._new_shared(numel, device=elem.device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 866, in _new_shared
    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\storage.py", line 260, in _new_shared
    return cls._new_using_filename_cpu(size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Couldn't open shared file mapping: <000001EC9AE53EC2>, error code: <1455>


The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=10776, ip=127.0.0.1, actor_id=a61260e8d671e45a7091d0df01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000273C7202A90>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marco\AppData\Local\Programs\Python\Python311\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 84, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client workload.\n\tClient 9 crashed when the DefaultActor was running its workload.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 72, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 191, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\client.py", line 223, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 227, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\client.py", line 68, in fit\n    train(self.model, self.trainloader, optim, epochs, self.device)\n  File "C:\\Users\\marco\\Documents\\Workplace\\FL\\Flower_Pytorch\\Parte_1\\model.py", line 39, in train\n    for images, labels in trainloader:\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1345, in _next_data\n    return self._process_data(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 1371, in _process_data\n    data.reraise()\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py", line 694, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py", line 160, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 866, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py", line 260, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn\'t open shared file mapping: <000001EC9AE53EC2>, error code: <1455>\n\n',)
[2024-11-21 16:54:33,418][flwr][DEBUG] - fit_round 5 received 9 results and 1 failures
[2024-11-21 16:54:40,067][flwr][INFO] - fit progress: (5, 33.43328568339348, {'accuracy': 0.8826}, 462.1822076999815)
[2024-11-21 16:54:40,067][flwr][INFO] - evaluate_round 5: no clients selected, cancel
[2024-11-21 16:54:40,067][flwr][DEBUG] - fit_round 6: strategy sampled 10 clients (out of 100)
[2024-11-21 16:55:50,385][flwr][DEBUG] - fit_round 6 received 10 results and 0 failures
[2024-11-21 16:55:56,236][flwr][INFO] - fit progress: (6, 25.605020336806774, {'accuracy': 0.9075}, 538.3513457999798)
[2024-11-21 16:55:56,237][flwr][INFO] - evaluate_round 6: no clients selected, cancel
[2024-11-21 16:55:56,238][flwr][DEBUG] - fit_round 7: strategy sampled 10 clients (out of 100)
[2024-11-21 16:57:01,968][flwr][DEBUG] - fit_round 7 received 10 results and 0 failures
[2024-11-21 16:57:07,807][flwr][INFO] - fit progress: (7, 20.5566533729434, {'accuracy': 0.9238}, 609.9214086999418)
[2024-11-21 16:57:07,807][flwr][INFO] - evaluate_round 7: no clients selected, cancel
[2024-11-21 16:57:07,808][flwr][DEBUG] - fit_round 8: strategy sampled 10 clients (out of 100)
[2024-11-21 16:58:14,857][flwr][DEBUG] - fit_round 8 received 10 results and 0 failures
[2024-11-21 16:58:21,470][flwr][INFO] - fit progress: (8, 17.044429775327444, {'accuracy': 0.9331}, 683.5845385999419)
[2024-11-21 16:58:21,471][flwr][INFO] - evaluate_round 8: no clients selected, cancel
[2024-11-21 16:58:21,472][flwr][DEBUG] - fit_round 9: strategy sampled 10 clients (out of 100)
[2024-11-21 16:59:34,110][flwr][DEBUG] - fit_round 9 received 10 results and 0 failures
[2024-11-21 16:59:41,810][flwr][INFO] - fit progress: (9, 15.59011384472251, {'accuracy': 0.9387}, 763.9250820999732)
[2024-11-21 16:59:41,811][flwr][INFO] - evaluate_round 9: no clients selected, cancel
[2024-11-21 16:59:41,812][flwr][DEBUG] - fit_round 10: strategy sampled 10 clients (out of 100)
[2024-11-21 17:00:49,224][flwr][DEBUG] - fit_round 10 received 10 results and 0 failures
[2024-11-21 17:00:55,997][flwr][INFO] - fit progress: (10, 14.24614988360554, {'accuracy': 0.9456}, 838.1116769999499)
[2024-11-21 17:00:55,998][flwr][INFO] - evaluate_round 10: no clients selected, cancel
[2024-11-21 17:00:55,998][flwr][INFO] - FL finished in 838.113043099991
[2024-11-21 17:00:55,999][flwr][INFO] - app_fit: losses_distributed []
[2024-11-21 17:00:56,000][flwr][INFO] - app_fit: metrics_distributed_fit {}
[2024-11-21 17:00:56,000][flwr][INFO] - app_fit: metrics_distributed {}
[2024-11-21 17:00:56,001][flwr][INFO] - app_fit: losses_centralized [(0, 182.52822732925415), (1, 179.48386120796204), (2, 164.87597584724426), (3, 68.83221188187599), (4, 50.320767253637314), (5, 33.43328568339348), (6, 25.605020336806774), (7, 20.5566533729434), (8, 17.044429775327444), (9, 15.59011384472251), (10, 14.24614988360554)]
[2024-11-21 17:00:56,002][flwr][INFO] - app_fit: metrics_centralized {'accuracy': [(0, 0.0894), (1, 0.2485), (2, 0.4898), (3, 0.739), (4, 0.8152), (5, 0.8826), (6, 0.9075), (7, 0.9238), (8, 0.9331), (9, 0.9387), (10, 0.9456)]}
[2024-11-21 17:00:56,023][__main__][INFO] - Aplicação iniciada
